{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HV4rVTmEg7D"
      },
      "source": [
        "**Imports**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "BzXavdjEEfpH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNgdpTAvEkxm"
      },
      "source": [
        "**Code:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jdNHe_rBFWk"
      },
      "source": [
        "Funzioni:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "AM4JmMYwBHN7"
      },
      "outputs": [],
      "source": [
        "def stampaPercentDF(df):\n",
        "    for name, group in df:\n",
        "    # La funzione value_counts() ritorna la percentuale di apparizione per ogni elemento univoco,\n",
        "    # è come un groupby però ci aggiunge le percentuali (se aggiungo il parametro normalize) altrimenti ritornebbe solo il numero di volte in cui i valori appaiono\n",
        "        group = round(group['Severity'].value_counts(normalize=True) * 100, 2)\n",
        "        print(name)\n",
        "        print(group)\n",
        "        print('-'*10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKKkH-e3JV_6"
      },
      "source": [
        "Dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "1JQm66jP1v3q"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv('medical-train.csv')\n",
        "df_test = pd.read_csv('medical-test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-STVLsuADMx"
      },
      "source": [
        "Elimino tutti i valori Nan dovuti dall'eliminazione dei dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "Gtq3qYw6ACel"
      },
      "outputs": [],
      "source": [
        "df_train = df_train[~(pd.isna(df_train.iloc[:,0]))]\n",
        "df_test = df_test[~(pd.isna(df_test.iloc[:,0]))]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z6qyVhkhAata"
      },
      "source": [
        "Drop di colonne / righe non utili allo studio dei dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "iZXMsNOMAQfA"
      },
      "outputs": [],
      "source": [
        "df_train = df_train.drop(df_train[df_train['Type'] == 'Moving average'].index, axis=0)\n",
        "df_train = df_train.drop(['Series_reference', 'Validation', 'Indicator', 'Data_value', 'Lower_CI', 'Upper_CI', 'Type'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test = df_test.drop(df_test[df_test['Type'] == 'Moving average'].index, axis=0)\n",
        "df_test = df_test.drop(['Series_reference', 'Validation', 'Indicator', 'Data_value', 'Lower_CI', 'Upper_CI', 'Type'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "E9gQusEnAoqK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Period' 'Units' 'Cause' 'Population' 'Age' 'Severity']\n",
            "['Period' 'Units' 'Cause' 'Population' 'Age' 'Severity']\n"
          ]
        }
      ],
      "source": [
        "print(df_train.columns.values)\n",
        "print(df_test.columns.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JzT2wuMA6Fx"
      },
      "source": [
        "Raggruppo i dati"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dati di Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "4oo7ObeJA1rs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Period  Units  Cause  Population  Age  Severity\n",
            "450    2000      1      1           2    3         1\n",
            "451    2001      1      1           2    3         1\n",
            "452    2002      1      1           2    3         1\n",
            "453    2003      1      1           2    3         1\n",
            "454    2004      1      1           2    3         1\n",
            "...     ...    ...    ...         ...  ...       ...\n",
            "1942   2012      1      4           1    1         3\n",
            "1943   2013      1      4           1    1         3\n",
            "1944   2014      1      4           1    1         3\n",
            "1945   2015      1      4           1    1         3\n",
            "1946   2016      1      4           1    1         3\n",
            "\n",
            "[1497 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "units_map = {\n",
        "    'Injuries': 1,\n",
        "    'Per 100,000 FTEs': 2,\n",
        "    'Per 100,000 people': 3,\n",
        "    'Per billion km': 4,\n",
        "    'Per thousand registered vehicles': 5\n",
        "}\n",
        "df_train['Units'] = df_train['Units'].map(units_map)\n",
        "\n",
        "# ! Abbiamo tirato via Indicator perchè le percentuali erano molto simili alla colonna Units e un analisi più approfondita ha rivelato che le stesse unità avevano per la maggior parte gli stessi indicatori\n",
        "\n",
        "pop_map = {\n",
        "    'Maori': 1,\n",
        "    'Whole pop': 2\n",
        "}\n",
        "df_train['Population'] = df_train['Population'].map(pop_map)\n",
        "\n",
        "cause_map = {\n",
        "    'All': 1,\n",
        "    'Assault': 2,\n",
        "    'Drowing': 3,\n",
        "    'Falls': 4,\n",
        "    'Intentional self-harm': 5,\n",
        "    'Motor vehicle traffic crashes': 6,\n",
        "    'Work': 7\n",
        "}\n",
        "df_train['Cause'] = df_train['Cause'].map(cause_map)\n",
        "\n",
        "age_map = {\n",
        "    '0-74 years': 1,\n",
        "    '75+ years': 2,\n",
        "    'All ages': 3\n",
        "}\n",
        "df_train['Age'] = df_train['Age'].map(age_map)\n",
        "\n",
        "severity_map = {\n",
        "    'Fatal': 1,\n",
        "    'Serious non-fatal': 2,\n",
        "    'Serious': 3\n",
        "}\n",
        "df_train['Severity'] = df_train['Severity'].map(severity_map)\n",
        "\n",
        "print(df_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dati di Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Units\n",
            "   Period  Units  Cause  Population  Age  Severity\n",
            "0    2017      1    4.0         1.0  1.0         3\n",
            "1    2018      1    4.0         1.0  1.0         3\n",
            "2    2000      3    4.0         1.0  1.0         3\n",
            "3    2001      3    4.0         1.0  1.0         3\n",
            "4    2002      3    4.0         1.0  1.0         3\n",
            "5    2003      3    4.0         1.0  1.0         3\n",
            "6    2004      3    4.0         1.0  1.0         3\n",
            "7    2005      3    4.0         1.0  1.0         3\n",
            "8    2006      3    4.0         1.0  1.0         3\n",
            "9    2007      3    4.0         1.0  1.0         3\n"
          ]
        }
      ],
      "source": [
        "print('Units')\n",
        "units_map = {\n",
        "    'Injuries': 1,\n",
        "    'Per 100,000 FTEs': 2,\n",
        "    'Per 100,000 people': 3,\n",
        "    'Per billion km': 4,\n",
        "    'Per thousand registered vehicles': 5\n",
        "}\n",
        "df_test['Units'] = df_test['Units'].map(units_map)\n",
        "\n",
        "# ! Abbiamo tirato via Indicator perchè le percentuali erano molto simili alla colonna Units e un analisi più approfondita ha rivelato che le stesse unità avevano per la maggior parte gli stessi indicatori\n",
        "\n",
        "pop_map = {\n",
        "    'Maori': 1,\n",
        "    'Whole pop': 2\n",
        "}\n",
        "df_test['Population'] = df_test['Population'].map(pop_map)\n",
        "\n",
        "cause_map = {\n",
        "    'All': 1,\n",
        "    'Assault': 2,\n",
        "    'Drowing': 3,\n",
        "    'Falls': 4,\n",
        "    'Intentional self-harm': 5,\n",
        "    'Motor vehicle traffic crashes': 6,\n",
        "    'Work': 7\n",
        "}\n",
        "df_test['Cause'] = df_test['Cause'].map(cause_map)\n",
        "\n",
        "age_map = {\n",
        "    '0-74 years': 1,\n",
        "    '75+ years': 2,\n",
        "    'All ages': 3\n",
        "}\n",
        "df_test['Age'] = df_test['Age'].map(age_map)\n",
        "\n",
        "severity_map = {\n",
        "    'Fatal': 1,\n",
        "    'Serious non-fatal': 2,\n",
        "    'Serious': 3\n",
        "}\n",
        "df_test['Severity'] = df_test['Severity'].map(severity_map)\n",
        "\n",
        "print(df_test.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He2nrvgyBMEB"
      },
      "source": [
        "Code in progress..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1497, 5), (1497,), (801, 5))"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Y_test = df_test[\"Severity\"]\n",
        "Y_train = df_train[\"Severity\"]\n",
        "X_train = df_train.drop(\"Severity\", axis=1)\n",
        "X_test  = df_test.drop(\"Severity\", axis=1,errors='ignore').copy()\n",
        "X_train.shape, Y_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "9-d_tPbrBLV3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     Period  Units  Cause  Population  Age\n",
            "0      2017      1    4.0         1.0  1.0\n",
            "1      2018      1    4.0         1.0  1.0\n",
            "2      2000      3    4.0         1.0  1.0\n",
            "3      2001      3    4.0         1.0  1.0\n",
            "4      2002      3    4.0         1.0  1.0\n",
            "..      ...    ...    ...         ...  ...\n",
            "796    2014      1    NaN         NaN  NaN\n",
            "797    2015      1    NaN         NaN  NaN\n",
            "798    2016      1    NaN         NaN  NaN\n",
            "799    2017      1    NaN         NaN  NaN\n",
            "800    2018      1    NaN         NaN  NaN\n",
            "\n",
            "[801 rows x 5 columns]\n",
            "42.89\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\marko\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[126], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m acc_log \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(logreg\u001b[39m.\u001b[39mscore(X_train, Y_train) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39mprint\u001b[39m(acc_log)\n\u001b[1;32m----> 6\u001b[0m acc_log \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(logreg\u001b[39m.\u001b[39;49mscore(X_test, Y_test) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m      8\u001b[0m acc_log\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:668\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 668\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_base.py:419\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39mPredict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    407\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m    Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    418\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 419\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecision_function(X)\n\u001b[0;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(scores\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    421\u001b[0m     indices \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(scores \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mint\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\linear_model\\_base.py:400\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    397\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    398\u001b[0m xp, _ \u001b[39m=\u001b[39m get_namespace(X)\n\u001b[1;32m--> 400\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    401\u001b[0m scores \u001b[39m=\u001b[39m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n\u001b[0;32m    402\u001b[0m \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39mreshape(scores, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m scores\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m scores\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    564\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 565\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    567\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m estimator_name \u001b[39mand\u001b[39;00m input_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n",
            "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, Y_train)\n",
        "print(X_test)\n",
        "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
        "print(acc_log)\n",
        "acc_log = round(logreg.score(X_test, Y_test) * 100, 2)\n",
        "\n",
        "acc_log"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
